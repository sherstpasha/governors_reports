{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартные библиотеки\n",
    "import copy\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Импорты из внешних пакетов (Detectron2)\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import load_coco_json\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import detectron2.data.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('detectron2')\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_train_path = 'data/train/annotations_train.json'\n",
    "annotations_validation_path = 'data/validation/annotations_validation.json'\n",
    "images_path = 'data/images'\n",
    "binary_mask_path = 'data/binary_mask_train.npz'\n",
    "output_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%d_%m_%y_%H_%M\")\n",
    "    return formatted_datetime\n",
    "    \n",
    "\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def check_gpu_availability():\n",
    "    if torch.cuda.is_available():    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def show_images(file_id, dataset, metadata):\n",
    "    example = dataset[file_id]\n",
    "    image = utils.read_image(example[\"file_name\"], format=\"RGB\")\n",
    "    plt.figure(figsize=(3,3),dpi=200)\n",
    "    visualizer = Visualizer(image[:, :, ::-1], metadata=metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(example)\n",
    "    plt.imshow(vis.get_image()[:, :,::-1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def f1_loss(y_actual, y_predicted):\n",
    "    true_positive = np.sum(y_actual & y_predicted)\n",
    "    false_positive = np.sum(~y_actual & y_predicted)\n",
    "    false_negative = np.sum(y_actual & ~y_predicted)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive + epsilon)\n",
    "    recall = true_positive / (true_positive + false_negative + epsilon)\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall + epsilon)\n",
    "    return f1\n",
    "\n",
    "\n",
    "class custom_mapper:\n",
    "    def __init__(self, cfg):\n",
    "        self.transform_list = [\n",
    "            T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],\n",
    "                cfg.INPUT.MAX_SIZE_TEST),\n",
    "            T.RandomBrightness(0.9, 1.1),\n",
    "            T.RandomContrast(0.9, 1.1),\n",
    "            T.RandomSaturation(0.9, 1.1),\n",
    "            T.RandomLighting(0.9)\n",
    "        ]\n",
    "        print(f\"[custom_mapper]: {self.transform_list}\")\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    \n",
    "        image, transforms = T.apply_transform_gens(self.transform_list, image)\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "        annos = [\n",
    "            utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "            for obj in dataset_dict.pop(\"annotations\")\n",
    "            if obj.get(\"iscrowd\", 0) == 0\n",
    "        ]\n",
    "\n",
    "        instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "        dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "        return dataset_dict\n",
    "\n",
    "\n",
    "CHECKPOINTS_RESULTS = []\n",
    "\n",
    "class F1Evaluator(DatasetEvaluator):\n",
    "    def __init__(self):\n",
    "        self.loaded_true = np.load(binary_mask_path)\n",
    "        self.val_predictions = {}\n",
    "        self.f1_scores = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val_predictions = {}\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            filename = input[\"file_name\"].split(\"/\")[-1]\n",
    "            if filename != \"41_3.JPG\":\n",
    "                true = self.loaded_true[filename].reshape(-1)\n",
    "\n",
    "                prediction = output['instances'].pred_masks.cpu().numpy()\n",
    "                mask = np.add.reduce(prediction)\n",
    "                mask = (mask > 0).reshape(-1)\n",
    "\n",
    "                self.f1_scores.append(f1_loss(true, mask))\n",
    "\n",
    "    def evaluate(self):\n",
    "        global CHECKPOINTS_RESULTS\n",
    "        result = np.mean(self.f1_scores)\n",
    "        CHECKPOINTS_RESULTS.append(result)\n",
    "        return {\"meanF1\": result}\n",
    "    \n",
    "\n",
    "class AugTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=custom_mapper(cfg))\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return F1Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetCatalog.register(\"my_dataset_train\",lambda: load_coco_json(annotations_train_path,\n",
    "                                                                  image_root = images_path,\n",
    "                                                                  dataset_name=\"my_dataset_train\",\n",
    "                                                                  extra_annotation_keys=['bbox_mode']))\n",
    "DatasetCatalog.register(\"my_dataset_validation\",lambda: load_coco_json(annotations_validation_path,\n",
    "                                                                  image_root = images_path,\n",
    "                                                                  dataset_name=\"my_dataset_validation\",\n",
    "                                                                  extra_annotation_keys=['bbox_mode']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DatasetCatalog.get(\"my_dataset_train\")\n",
    "dataset_validation = DatasetCatalog.get(\"my_dataset_validation\")\n",
    "print(f'Training dataset size (Images): {len(dataset_train)}')\n",
    "\n",
    "metadata_train = MetadataCatalog.get(\"my_dataset_train\")\n",
    "metadata_validation = MetadataCatalog.get(\"my_dataset_validation\")\n",
    "print(f'Validation dataset size (Images): {len(dataset_validation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(file_id=0, dataset=dataset_train, metadata=metadata_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")) \n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
    "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
    "\n",
    "# Parallel data loading\n",
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "\n",
    "# Image compression\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 2160\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 3130\n",
    "cfg.INPUT.MIN_SIZE_TEST = cfg.INPUT.MIN_SIZE_TRAIN\n",
    "cfg.INPUT.MAX_SIZE_TEST = cfg.INPUT.MAX_SIZE_TRAIN\n",
    "\n",
    "# Decision threshold\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1\n",
    "\n",
    "# Validation frequency\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "\n",
    "# Checkpoint frequency\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = cfg.TEST.EVAL_PERIOD\n",
    "\n",
    "# Blue Green Red (BGR) channel format\n",
    "cfg.INPUT.FORMAT = 'BGR' \n",
    "\n",
    "# Batch size\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1\n",
    "\n",
    "# Learning rate\n",
    "cfg.SOLVER.BASE_LR = 0.01\n",
    "\n",
    "# Learning rate reduction frequency\n",
    "cfg.SOLVER.STEPS = (1500,)\n",
    "\n",
    "# Learning rate reduction factor\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "\n",
    "# Number of training iterations\n",
    "cfg.SOLVER.MAX_ITER = 17000\n",
    "\n",
    "# Number of classes in the dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "# Maximum number of words per page\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "\n",
    "# Model output path\n",
    "time = get_time()\n",
    "cfg.OUTPUT_DIR = output_path + '/output' + time\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AugTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(enumerate(CHECKPOINTS_RESULTS, start=1))\n",
    "with open(\"CHECKPOINTS_RESULTS.txt\", \"w\") as f:\n",
    "    f.write(str(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных из JSON-файла\n",
    "with open(output_path + '/output' + time + '/metrics.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Извлекаем список ключей из первой записи (можно использовать любую другую запись)\n",
    "all_keys = list(data[0].keys())\n",
    "\n",
    "# Убираем ключи, которые не подходят для построения графика (например, \"data_time\" и \"eta_seconds\")\n",
    "keys_for_plotting = [key for key in all_keys if key not in [\"data_time\", \"eta_seconds\"]]\n",
    "\n",
    "# Группируем ключи по принадлежности к определенной группе\n",
    "fast_rcnn_keys = [\"fast_rcnn/cls_accuracy\", \"fast_rcnn/false_negative\", \"fast_rcnn/fg_cls_accuracy\"]\n",
    "mask_rcnn_keys = [\"mask_rcnn/accuracy\", \"mask_rcnn/false_negative\", \"mask_rcnn/false_positive\"]\n",
    "loss_keys = ['loss_box_reg', 'loss_cls', 'loss_mask', 'loss_rpn_cls', 'loss_rpn_loc']\n",
    "\n",
    "# Итерируемся по группам ключей и строим графики\n",
    "for keys, title in zip([fast_rcnn_keys, mask_rcnn_keys, loss_keys], [\"Fast R-CNN Metrics\", \"Mask R-CNN Metrics\", \"Loss\"]):\n",
    "    for key in keys:\n",
    "        values = []\n",
    "\n",
    "        # Итерируемся по данным и добавляем значения для текущего ключа в список\n",
    "        for entry in data:\n",
    "            value = entry.get(key)\n",
    "            if value is not None:\n",
    "                values.append(value)\n",
    "\n",
    "        # Строим график\n",
    "        plt.plot(range(1, len(values) + 1), values, marker='o', label=key)\n",
    "\n",
    "    plt.xlabel('Record Number')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fast R-CNN и Mask R-CNN**\n",
    "\n",
    "*Fast R-CNN и Mask R-CNN* - это две эволюционные модификации архитектуры *R-CNN* (Region-based Convolutional Neural Network), предназначенные для решения задачи объектной детекции и сегментации объектов на изображениях. Вот основные отличия между ними:\n",
    "\n",
    "**Цель задачи:**\n",
    "- **Fast R-CNN:** Основной целью Fast R-CNN является детекция объектов на изображениях. Он предлагает более эффективный способ извлечения признаков и классификации регионов изображения.\n",
    "- **Mask R-CNN:** Помимо детекции объектов, Mask R-CNN также решает задачу сегментации объектов. То есть, помимо классификации и определения ограничивающих прямоугольников, он генерирует маски для каждого объекта, указывая пиксели, принадлежащие объекту.\n",
    "\n",
    "**Архитектура:**\n",
    "- **Fast R-CNN:** Использует Region Proposal Network (RPN) для предложения областей с потенциальными объектами, а затем применяет сверточные слои и ROI pooling для извлечения признаков и классификации.\n",
    "- **Mask R-CNN:** Расширяет архитектуру Fast R-CNN, добавляя дополнительную ветвь для генерации масок. Эта ветвь генерирует маску для каждого объекта в ROI (Region of Interest).\n",
    "\n",
    "**Архитектура и Задачи Fast R-CNN и Mask R-CNN**\n",
    "\n",
    "*Fast R-CNN:* Использует Region Proposal Network (RPN) для предложения областей с потенциальными объектами, а затем применяет сверточные слои и ROI pooling для извлечения признаков и классификации.\n",
    "\n",
    "*Mask R-CNN:* Расширяет архитектуру Fast R-CNN, добавляя дополнительную ветвь для генерации масок. Эта ветвь генерирует маску для каждого объекта в ROI (Region of Interest).\n",
    "\n",
    "**Задачи, решаемые моделями:**\n",
    "- *Fast R-CNN:* Решает задачу детекции объектов и классификации.\n",
    "- *Mask R-CNN:* Расширяет функциональность Fast R-CNN, добавляя задачу сегментации объектов (генерация масок).\n",
    "\n",
    "**Процесс работы с масками:**\n",
    "- *Fast R-CNN:* Не генерирует маски объектов, а только ограничивающие прямоугольники.\n",
    "- *Mask R-CNN:* Помимо ограничивающих прямоугольников, генерирует маски для каждого объекта, позволяя точно определить пиксели, принадлежащие объекту.\n",
    "\n",
    "**Производительность:**\n",
    "\n",
    "- *Fast R-CNN:* Более быстрый по сравнению с предыдущей версией R-CNN за счет использования ROI pooling, что позволяет эффективнее извлекать признаки из регионов интереса.\n",
    "\n",
    "- *Mask R-CNN:* Более затратный с точки зрения вычислений из-за генерации масок, что делает его медленнее Fast R-CNN. Однако, он предоставляет дополнительную информацию о форме объектов и их расположении на уровне пикселей.\n",
    "\n",
    "Выбор между Fast R-CNN и Mask R-CNN зависит от конкретных требований задачи. Если требуется не только детекция объектов, но и точная сегментация, то Mask R-CNN предоставляет необходимую функциональность за счет добавления задачи генерации масок. Однако, следует учитывать увеличение вычислительной сложности и времени обучения при использовании Mask R-CNN.\n",
    "\n",
    "**Fast R-CNN Metrics:**\n",
    "1. **`fast_rcnn/cls_accuracy` (Fast R-CNN Classification Accuracy):**\n",
    "   - Эта метрика измеряет точность классификации объектов моделью Fast R-CNN. Точность классификации показывает, как хорошо модель правильно классифицирует объекты среди всех обнаруженных объектов.\n",
    "\n",
    "2. **`fast_rcnn/false_negative` (Fast R-CNN False Negatives):**\n",
    "   - Эта метрика измеряет количество ложных отрицательных предсказаний модели Fast R-CNN. Ложные отрицательные предсказания возникают, когда модель не обнаруживает объекты, которые на самом деле присутствуют в изображении.\n",
    "\n",
    "3. **`fast_rcnn/fg_cls_accuracy` (Fast R-CNN Foreground Classification Accuracy):**\n",
    "   - Данная метрика измеряет точность классификации объектов, которые модель Fast R-CNN рассматривает как передний план (foreground). Передний план обычно относится к регионам изображения, содержащим объекты.\n",
    "\n",
    "**Mask R-CNN Metrics:**\n",
    "1. **`mask_rcnn/accuracy` (Mask R-CNN Accuracy):**\n",
    "   - Эта метрика измеряет общую точность модели Mask R-CNN, включая как классификацию объектов, так и точность сегментации масок. Она оценивает, насколько хорошо модель выполняет обе части задачи.\n",
    "\n",
    "2. **`mask_rcnn/false_negative` (Mask R-CNN False Negatives):**\n",
    "   - Эта метрика измеряет количество ложных отрицательных предсказаний для сегментации. Такие предсказания возникают, когда модель не правильно выделяет области объектов на изображении.\n",
    "\n",
    "3. **`mask_rcnn/false_positive` (Mask R-CNN False Positives):**\n",
    "   - Метрика измеряет количество ложных положительных предсказаний для сегментации. Ложные положительные предсказания возникают, когда модель неверно выделяет области, которые не являются частью объекта.\n",
    "\n",
    "Эти метрики позволяют оценить качество работы моделей Fast R-CNN и Mask R-CNN с точки зрения классификации объектов и сегментации масок.\n",
    "\n",
    "\n",
    "**Loss:**\n",
    "1. **`loss_box_reg` (Loss for bounding box regression):**\n",
    "   - Этот лосс измеряет ошибку между предсказанными и истинными координатами ограничивающего прямоугольника (bounding box) объекта. Модель предсказывает координаты (x, y, width, height) ограничивающего прямоугольника, и `loss_box_reg` измеряет, насколько эти предсказанные координаты отличаются от фактических.\n",
    "\n",
    "2. **`loss_cls` (Classification Loss):**\n",
    "   - Этот лосс отвечает за классификацию объектов в изображении. Модель предсказывает вероятности принадлежности объектов к различным классам, и `loss_cls` измеряет разницу между предсказанными вероятностями и фактическими метками классов.\n",
    "\n",
    "3. **`loss_mask` (Mask Prediction Loss):**\n",
    "   - Для сегментации объектов (например, выделение объектов пикселями) используется `loss_mask`. Модель предсказывает маску каждого объекта, и этот лосс измеряет разницу между предсказанной маской и фактической маской.\n",
    "\n",
    "4. **`loss_rpn_cls` (Region Proposal Network Classification Loss):**\n",
    "   - Faster R-CNN использует Region Proposal Network (RPN) для предложения областей, где могут находиться объекты. `loss_rpn_cls` измеряет ошибку в классификации предложенных областей (пропозалов) на объекты и не-объекты.\n",
    "\n",
    "5. **`loss_rpn_loc` (Region Proposal Network Localization Loss):**\n",
    "   - Этот лосс отвечает за определение координат ограничивающих прямоугольников для предложенных областей (пропозалов), созданных RPN. Он измеряет ошибку в предсказании координат ограничивающих прямоугольников.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
